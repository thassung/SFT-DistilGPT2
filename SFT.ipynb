{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5896b1a6-8ed4-4484-96e5-50d0ac10b73a",
   "metadata": {},
   "source": [
    "# [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)\n",
    "\n",
    "Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "[Python Script](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f636b5-91b3-4a45-a6cf-334425eac4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install peft==0.7.1\n",
    "# !pip3 install trl==0.7.4\n",
    "# !pip3 install transformers==4.36.2\n",
    "# !pip3 install pydantic==1.10.9\n",
    "# !pip3 install datasets==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd24274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sung2_8l7o06c\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7263f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sung2_8l7o06c\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.7.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ed1948-2b9b-4324-ba26-36b6c95fdbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16f67f1-c0e4-40e9-b192-4d1a9cfbfb17",
   "metadata": {},
   "source": [
    "## Instruction-Tuning\n",
    "Train on completions only\n",
    "- Use the DataCollatorForCompletionOnlyLM to train your model on the generated prompts only.\n",
    "- Note that this works only in the case when packing=False.\n",
    "- To instantiate that collator for instruction data, pass a response template and the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1ea5ec-482c-4520-bd97-3ccc1f2961f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "from datasets import load_dataset\n",
    "train_json_url = 'https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json'\n",
    "dataset = load_dataset(\"json\", data_files=train_json_url)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a45134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sung2_8l7o06c\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for tatsu-lab/alpaca_eval contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tatsu-lab/alpaca_eval\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['instruction', 'output', 'generator', 'dataset'],\n",
       "        num_rows: 805\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['eval'] = load_dataset('tatsu-lab/alpaca_eval', split='eval')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6ee677-ddc8-4a81-9d86-9219de3d84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Name the given musical note.',\n",
       " 'input': '(A musical note)',\n",
       " 'output': 'The musical note is an F sharp.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5fb3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'I like to host guests at my home from time to time, and I am gathering  recipes of different dishes and drinks to keep things interesting. I am interested in trying some Indonesian dishes. Can you give me a recipe for Tahu Gejrot Cirebon?',\n",
       " 'output': 'Ingredients: \\n- 2 tablespoons of sweet soy sauce \\n- 2 tablespoons of chili sauce \\n- 2 tablespoons of vinegar \\n- 2 tablespoons of sugar \\n- 1 tablespoon of ground ginger \\n- 2 tablespoons of vegetable oil \\n- 2 cloves of garlic, minced \\n- 1/4 teaspoon of ground pepper \\n- 1/2 teaspoon of ground cumin \\n- 1/4 teaspoon of ground nutmeg \\n- 2 tablespoons of tomato paste \\n- 2 packages of firm tofu, cut into cubes \\n- 2 tablespoons of chopped shallots\\n- 2 tablespoons of chopped scallions\\n- 2 tablespoons of chopped celery\\n- 2 tablespoons of chopped chilies\\n\\nInstructions:\\n1. In a medium bowl, mix together the sweet soy sauce, chili sauce, vinegar, sugar, ground ginger, vegetable oil, garlic, ground pepper, ground cumin, and ground nutmeg. \\n2. Heat the tomato paste in a large skillet over medium-high heat. \\n3. Add the tofu to the skillet and stir to coat. \\n4. Pour the sauce mixture over the tofu and stir to coat. \\n5. Add the shallots, scallions, celery, and chilies and stir to combine. \\n6. Cook until the tofu is golden brown and crispy, about 10 minutes. \\n7. Serve hot with steamed white rice or freshly cooked noodles. Enjoy!',\n",
       " 'generator': 'text_davinci_003',\n",
       " 'dataset': 'helpful_base'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['eval'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69100196-d9d8-4791-9e11-6e93f1bd7550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Load the model & Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name_or_path = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e04e135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ad760",
   "metadata": {},
   "source": [
    "### Format prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44ecf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "for i in dataset['train']:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5daf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77fe084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in dataset['eval']:\n",
    "    break\n",
    "sample['input'] if 'input' in sample.keys() else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81d350a2-002b-40e2-8c10-9afea5923cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Input:\\n\\n\\n### Output:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Input:\\n\\n\\n### Output:\\nThe three primary colors are red, blue, and yellow.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_instruction(examples):\n",
    "\toutputs = []\n",
    "\tfor i in range(len(examples['instruction'])):\n",
    "\t\tinput = examples['input'][i] if 'input' in examples.keys() else ''\n",
    "\t\toutputs.append(f\"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{examples['instruction'][i]}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Output:\n",
    "{examples['output'][i]}\n",
    "\"\"\".strip())\n",
    "\n",
    "\treturn outputs\n",
    "\t\n",
    "format_instruction(dataset['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83f5702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['Give three tips for staying healthy.',\n",
       "  'What are the three primary colors?'],\n",
       " 'input': ['', ''],\n",
       " 'output': ['1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       "  'The three primary colors are red, blue, and yellow.']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46cdf58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Output:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "do you think retinoid is effective on removing the acne? because I have a lot of it\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Output:\n",
      "Yes, retinoids are effective in treating acne. They work by increasing cell turnover, which helps to reduce the appearance of existing acne and prevent new outbreaks. Retinoids also help to unclog pores, which in turn reduces the amount of bacteria that can cause infections. In general, retinoids help to reduce inflammation and oil production, making them a great option for those with acne.\n"
     ]
    }
   ],
   "source": [
    "print(format_instruction(dataset['train'][[0]])[0])\n",
    "print('\\n'+'='*50+'\\n')\n",
    "print(format_instruction(dataset['eval'][[10]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a38e5c61-8397-4fd6-bcc2-b5c8c929b759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForCompletionOnlyLM(tokenizer=GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the DataCollatorForCompletionOnlyLM to train your model on the generated prompts only\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "response_template = \"### Output:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "452f5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38b7259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28385087-8eb8-4b83-a7dd-1313bf591b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 805/805 [00:00<00:00, 3374.38 examples/s]\n",
      "c:\\Users\\sung2_8l7o06c\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/625 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 20%|██        | 125/625 [01:23<04:47,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6941, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 20%|██        | 125/625 [02:33<04:47,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2960920333862305, 'eval_runtime': 70.0126, 'eval_samples_per_second': 11.498, 'eval_steps_per_second': 1.443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 250/625 [04:34<03:43,  1.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3693, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 250/625 [05:39<03:43,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.305957555770874, 'eval_runtime': 65.0135, 'eval_samples_per_second': 12.382, 'eval_steps_per_second': 1.554, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 375/625 [07:34<02:13,  1.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1768, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 375/625 [08:35<02:13,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3381845951080322, 'eval_runtime': 60.3404, 'eval_samples_per_second': 13.341, 'eval_steps_per_second': 1.674, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 500/625 [10:35<01:16,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0642, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 500/625 [11:39<01:16,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3685972690582275, 'eval_runtime': 64.3247, 'eval_samples_per_second': 12.515, 'eval_steps_per_second': 1.57, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [13:39<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9873, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 625/625 [14:45<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3767008781433105, 'eval_runtime': 66.3878, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 1.521, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [14:51<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 892.4502, 'train_samples_per_second': 5.603, 'train_steps_per_second': 0.7, 'train_loss': 2.258316552734375, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=2.258316552734375, metrics={'train_runtime': 892.4502, 'train_samples_per_second': 5.603, 'train_steps_per_second': 0.7, 'train_loss': 2.258316552734375, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Define the Trainer\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{file_path}/model', #default = 'tmp_trainer'\n",
    "    num_train_epochs=5, #default = 3\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8 ,\n",
    "    logging_steps=1,\n",
    "    logging_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'].select(range(1000)),\n",
    "    eval_dataset=dataset['eval'],\n",
    "    formatting_func=format_instruction,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c6aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf4520",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c974ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_path = f'{file_path}/model/checkpoint-625'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map = 'auto')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path)\n",
    "\n",
    "text_generator = pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map = 'auto',\n",
    "    # Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "448121f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Output:\n"
     ]
    }
   ],
   "source": [
    "def format_singular_prompt(sample):\n",
    "    input = sample['input'] if 'input' in sample.keys() else ''\n",
    "    return f\"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Output:\n",
    "\"\"\".strip()\n",
    "\n",
    "\t\n",
    "print(format_singular_prompt(dataset['train'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fba3e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def compare_output(sample, max_length=125, temperature=0.7):\n",
    "    generated_text = text_generator(format_singular_prompt(sample),\n",
    "                                    max_length = max_length,\n",
    "                                    temperature = temperature)\n",
    "    generated_text = generated_text[0]['generated_text']\n",
    "    generated_output = generated_text.split('### Output:\\n')[-1]\n",
    "    html_content = f\"\"\"\n",
    "    <p><b>Instruction:</b></p>\n",
    "    <p>{sample['instruction']}</p>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>Gold output</td>\n",
    "            <td>Generated output</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>{sample['output']}</td>\n",
    "            <td>{generated_output}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n",
    "    # del generated_text\n",
    "    # del generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b82ce40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><b>Instruction:</b></p>\n",
       "    <p>do you think retinoid is effective on removing the acne? because I have a lot of it</p>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <td>Gold output</td>\n",
       "            <td>Generated output</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Yes, retinoids are effective in treating acne. They work by increasing cell turnover, which helps to reduce the appearance of existing acne and prevent new outbreaks. Retinoids also help to unclog pores, which in turn reduces the amount of bacteria that can cause infections. In general, retinoids help to reduce inflammation and oil production, making them a great option for those with acne.</td>\n",
       "            <td>I think retinoid can help reduce the risk of my acne. It helps reduce the risk of my acne by reducing the risk of my skin becoming acne-free. Additionally, retinoids can help reduce the risk of my skin becoming acne-free. Additionally, retinoids can help reduce the risk of my skin becoming</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_output(dataset['eval'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c21ae70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><b>Instruction:</b></p>\n",
       "    <p>I like to host guests at my home from time to time, and I am gathering  recipes of different dishes and drinks to keep things interesting. I am interested in trying some Indonesian dishes. Can you give me a recipe for Tahu Gejrot Cirebon?</p>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <td>Gold output</td>\n",
       "            <td>Generated output</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Ingredients: \n",
       "- 2 tablespoons of sweet soy sauce \n",
       "- 2 tablespoons of chili sauce \n",
       "- 2 tablespoons of vinegar \n",
       "- 2 tablespoons of sugar \n",
       "- 1 tablespoon of ground ginger \n",
       "- 2 tablespoons of vegetable oil \n",
       "- 2 cloves of garlic, minced \n",
       "- 1/4 teaspoon of ground pepper \n",
       "- 1/2 teaspoon of ground cumin \n",
       "- 1/4 teaspoon of ground nutmeg \n",
       "- 2 tablespoons of tomato paste \n",
       "- 2 packages of firm tofu, cut into cubes \n",
       "- 2 tablespoons of chopped shallots\n",
       "- 2 tablespoons of chopped scallions\n",
       "- 2 tablespoons of chopped celery\n",
       "- 2 tablespoons of chopped chilies\n",
       "\n",
       "Instructions:\n",
       "1. In a medium bowl, mix together the sweet soy sauce, chili sauce, vinegar, sugar, ground ginger, vegetable oil, garlic, ground pepper, ground cumin, and ground nutmeg. \n",
       "2. Heat the tomato paste in a large skillet over medium-high heat. \n",
       "3. Add the tofu to the skillet and stir to coat. \n",
       "4. Pour the sauce mixture over the tofu and stir to coat. \n",
       "5. Add the shallots, scallions, celery, and chilies and stir to combine. \n",
       "6. Cook until the tofu is golden brown and crispy, about 10 minutes. \n",
       "7. Serve hot with steamed white rice or freshly cooked noodles. Enjoy!</td>\n",
       "            <td>I like to host guests at my home from time to time, and I am gathering  recipes of different dishes and drinks to keep things interesting. I am interested in trying some Indonesian dishes. Can you give me a recipe for Tahu Gejrot Cirebon? \n",
       "Ingredients: \n",
       "1. Pre-baked \n",
       "2. Oat-like \n",
       "3. Tapioca \n",
       "4. Cumin \n",
       "5. Stir in \n",
       "6. Coconut oil \n",
       "7. Fruit. \n",
       "8. Salt\n",
       "9. Banana Juice \n",
       "10. Bananjali \n",
       "11. Bananjali \n",
       "12.  Simmer 1 tablespoon coconut oil. \n",
       "13.       \n",
       "14.      \n",
       "15.         \n",
       "16.       \n",
       "17.       \n",
       "18.     \n",
       "19. </td>\n",
       "        </tr>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_output(sample=dataset['eval'][100], max_length=300, temperature=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
